{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] 系统找不到指定的路径。: '金瓶梅.tokenized'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c64fa81f1f0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0minputfolder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"金瓶梅.tokenized\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0minputfiles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputfolder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m     \u001b[1;31m# Store the documents in a list\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [WinError 3] 系统找不到指定的路径。: '金瓶梅.tokenized'"
     ]
    }
   ],
   "source": [
    "#1\n",
    "import os\n",
    "\n",
    "inputfolder = \"金瓶梅.tokenized\"\n",
    "inputfiles = os.listdir(inputfolder)\n",
    "\n",
    "documents = []     # Store the documents in a list\n",
    "documentnames = [] # Also store the filenames in a list \n",
    "for file in inputfiles:\n",
    "    infile = open(inputfolder + \"/\" + file, \"r\", encoding=\"utf-8\")\n",
    "    documents.append(infile.read().split())\n",
    "    documentnames.append(file)\n",
    "\n",
    "file = open(\"金瓶梅tokenized.txt\", \"w\", encoding=\"utf-8\")\n",
    "for x in range (0,len(documents)):\n",
    "    for y in range (0,len(documents[x])):\n",
    "        file.write(str(documents[x][y]) + \" \")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "inputfolder = \"红楼梦.tokenized\"\n",
    "inputfiles = os.listdir(inputfolder)\n",
    "\n",
    "documents = []     # Store the documents in a list\n",
    "documentnames = [] # Also store the filenames in a list \n",
    "for file in inputfiles:\n",
    "    infile = open(inputfolder + \"/\" + file, \"r\", encoding=\"utf-8\")\n",
    "    documents.append(infile.read().split())\n",
    "    documentnames.append(file)\n",
    "\n",
    "file = open(\"红楼梦tokenized.txt\", \"w\", encoding=\"utf-8\")\n",
    "for x in range (0,len(documents)):\n",
    "    for y in range (0,len(documents[x])):\n",
    "        file.write(str(documents[x][y]) + \" \")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#3\n",
    "import os\n",
    "\n",
    "inputfolder = \"醒世姻缘传.tokenized\"\n",
    "inputfiles = os.listdir(inputfolder)\n",
    "\n",
    "documents = []     # Store the documents in a list\n",
    "documentnames = [] # Also store the filenames in a list \n",
    "for file in inputfiles:\n",
    "    infile = open(inputfolder + \"/\" + file, \"r\", encoding=\"utf-8\")\n",
    "    documents.append(infile.read().split())\n",
    "    documentnames.append(file)\n",
    "\n",
    "file = open(\"醒世姻缘传tokenized.txt\", \"w\", encoding=\"utf-8\")\n",
    "for x in range (0,len(documents)):\n",
    "    for y in range (0,len(documents[x])):\n",
    "        file.write(str(documents[x][y]) + \" \")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#4\n",
    "import os\n",
    "\n",
    "inputfolder = \"鼓掌绝尘.tokenized\"\n",
    "inputfiles = os.listdir(inputfolder)\n",
    "\n",
    "documents = []     # Store the documents in a list\n",
    "documentnames = [] # Also store the filenames in a list \n",
    "for file in inputfiles:\n",
    "    infile = open(inputfolder + \"/\" + file, \"r\", encoding=\"utf-8\")\n",
    "    documents.append(infile.read().split())\n",
    "    documentnames.append(file)\n",
    "\n",
    "file = open(\"鼓掌绝尘tokenized.txt\", \"w\", encoding=\"utf-8\")\n",
    "for x in range (0,len(documents)):\n",
    "    for y in range (0,len(documents[x])):\n",
    "        file.write(str(documents[x][y]) + \" \")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#5\n",
    "import os\n",
    "\n",
    "inputfolder = \"娇红记.tokenized\"\n",
    "inputfiles = os.listdir(inputfolder)\n",
    "\n",
    "documents = []     # Store the documents in a list\n",
    "documentnames = [] # Also store the filenames in a list \n",
    "for file in inputfiles:\n",
    "    infile = open(inputfolder + \"/\" + file, \"r\", encoding=\"utf-8\")\n",
    "    documents.append(infile.read().split())\n",
    "    documentnames.append(file)\n",
    "\n",
    "file = open(\"娇红记tokenized.txt\", \"w\", encoding=\"utf-8\")\n",
    "for x in range (0,len(documents)):\n",
    "    for y in range (0,len(documents[x])):\n",
    "        file.write(str(documents[x][y]) + \" \")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#6\n",
    "import os\n",
    "\n",
    "inputfolder = \"金云翘传.tokenized\"\n",
    "inputfiles = os.listdir(inputfolder)\n",
    "\n",
    "documents = []     # Store the documents in a list\n",
    "documentnames = [] # Also store the filenames in a list \n",
    "for file in inputfiles:\n",
    "    infile = open(inputfolder + \"/\" + file, \"r\", encoding=\"utf-8\")\n",
    "    documents.append(infile.read().split())\n",
    "    documentnames.append(file)\n",
    "\n",
    "file = open(\"金云翘传tokenized.txt\", \"w\", encoding=\"utf-8\")\n",
    "for x in range (0,len(documents)):\n",
    "    for y in range (0,len(documents[x])):\n",
    "        file.write(str(documents[x][y]) + \" \")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#7\n",
    "import os\n",
    "\n",
    "inputfolder = \"平山冷燕.tokenized\"\n",
    "inputfiles = os.listdir(inputfolder)\n",
    "\n",
    "documents = []     # Store the documents in a list\n",
    "documentnames = [] # Also store the filenames in a list \n",
    "for file in inputfiles:\n",
    "    infile = open(inputfolder + \"/\" + file, \"r\", encoding=\"utf-8\")\n",
    "    documents.append(infile.read().split())\n",
    "    documentnames.append(file)\n",
    "\n",
    "file = open(\"平山冷燕tokenized.txt\", \"w\", encoding=\"utf-8\")\n",
    "for x in range (0,len(documents)):\n",
    "    for y in range (0,len(documents[x])):\n",
    "        file.write(str(documents[x][y]) + \" \")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#8\n",
    "import os\n",
    "\n",
    "inputfolder = \"镜花缘.tokenized\"\n",
    "inputfiles = os.listdir(inputfolder)\n",
    "\n",
    "documents = []     # Store the documents in a list\n",
    "documentnames = [] # Also store the filenames in a list \n",
    "for file in inputfiles:\n",
    "    infile = open(inputfolder + \"/\" + file, \"r\", encoding=\"utf-8\")\n",
    "    documents.append(infile.read().split())\n",
    "    documentnames.append(file)\n",
    "\n",
    "file = open(\"镜花缘tokenized.txt\", \"w\", encoding=\"utf-8\")\n",
    "for x in range (0,len(documents)):\n",
    "    for y in range (0,len(documents[x])):\n",
    "        file.write(str(documents[x][y]) + \" \")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#9\n",
    "import os\n",
    "\n",
    "inputfolder = \"玉娇梨.tokenized\"\n",
    "inputfiles = os.listdir(inputfolder)\n",
    "\n",
    "documents = []     # Store the documents in a list\n",
    "documentnames = [] # Also store the filenames in a list \n",
    "for file in inputfiles:\n",
    "    infile = open(inputfolder + \"/\" + file, \"r\", encoding=\"utf-8\")\n",
    "    documents.append(infile.read().split())\n",
    "    documentnames.append(file)\n",
    "\n",
    "file = open(\"玉娇梨tokenized.txt\", \"w\", encoding=\"utf-8\")\n",
    "for x in range (0,len(documents)):\n",
    "    for y in range (0,len(documents[x])):\n",
    "        file.write(str(documents[x][y]) + \" \")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#10\n",
    "import os\n",
    "\n",
    "inputfolder = \"再生缘.tokenized\"\n",
    "inputfiles = os.listdir(inputfolder)\n",
    "\n",
    "documents = []     # Store the documents in a list\n",
    "documentnames = [] # Also store the filenames in a list \n",
    "for file in inputfiles:\n",
    "    infile = open(inputfolder + \"/\" + file, \"r\", encoding=\"utf-8\")\n",
    "    documents.append(infile.read().split())\n",
    "    documentnames.append(file)\n",
    "\n",
    "file = open(\"再生缘tokenized.txt\", \"w\", encoding=\"utf-8\")\n",
    "for x in range (0,len(documents)):\n",
    "    for y in range (0,len(documents[x])):\n",
    "        file.write(str(documents[x][y]) + \" \")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['再生缘tokenized.txt', '娇红记tokenized.txt', '平山冷燕tokenized.txt', '玉娇梨tokenized.txt', '红楼梦tokenized.txt', '醒世姻缘传tokenized.txt', '金云翘传tokenized.txt', '金瓶梅tokenized.txt', '镜花缘tokenized.txt', '鼓掌绝尘tokenized.txt']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "inputfolder = \"全小说tokenized\"\n",
    "inputfiles = os.listdir(inputfolder)\n",
    "\n",
    "\n",
    "documents = []     # Store the documents in a list\n",
    "documentnames = [] # Also store the filenames in a list \n",
    "for file in inputfiles:\n",
    "    infile = open(inputfolder + \"/\" + file, \"r\", encoding=\"utf-8\")\n",
    "    documents.append(infile.read().split())\n",
    "    documentnames.append(file)\n",
    "\n",
    "print(documentnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "df = {} # One frequency value for each term appearing in any document\n",
    "\n",
    "for doc in range(0, len(documents)):\n",
    "    for term in documents[doc]:\n",
    "        if term not in df:\n",
    "            df[term] = 0\n",
    "            for docnum in range(0, len(documents)):\n",
    "                if term in documents[docnum]:\n",
    "                    df[term] = df[term] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['归', '不然', '话', '主意', '如雨', '回话', '母', '而', '从前', '凭', '助', '性', '权', '今', '不及', '消息', '八', '不过', '前生', '便了', '急', '以此', '弃', '进来', '愁', '雨', '钩', '非', '前事', '中人', '以', '消', '书房', '烧', '约', '而行', '早', '闻知', '些', '文章', '那日', '酒肴', '解', '那', '散', '日日', '慌忙', '手', '暗', '起', '相见', '儿', '取', '无事', '富贵', '得', '想来', '方可', '郡', '方才', '明白', '肚', '调', '脚', '这等', '不起', '谨', '忘了', '何故', '「', '自来', '名字', '这', '地方', '英', '细看', '其人', '展', '远', '不成', '计较', '转身', '怪不得', '处处', '成', '淋漓', '父母', '所以', '海棠', '长叹', '清', '磨', '当日', '没', '一会', '买', '好事', '女', '山', '倒是', '晚', '锦', '采', '昨', '无奈', '夫妻', '红', '打', '先', '送上', '辈', '备', '主人', '末', '山川', '异', '死', '赋', '在外', '趁', '匆匆', '府', '原是', '早知', '写', '则', '转眼', '鼓', '儿女', '花', '位', '年年', '老爷', '不要', '情', '恐', '折', '这些', '正', '价', '出来', '人情', '志气', '帖', '未必', '为妻', '君', '子弟', '在手', '是', '对', '打听', '为何', '何如', '自然', '意', '正在', '可怜', '随他', '两', '男子', '似', '受用', '生出', '鱼', '当时', '挂', '多', '老', '争', '工夫', '楼上', '赏', '至', '小弟', '卜', '固', '父子', '目', '旗', '苏', '良', '么', '房', '才子', '宽', '炉', '还有', '身', '有的', '读书', '天气', '乐', '晓', '半', '三千', '枝', '最', '罗', '弄', '秋', '读', '时节', '卷', '部', '物', '性命', '鸟', '打发', '以为', '鬼', '跟', '佳人', '饭', '望', '妙', '送礼', '便宜', '速', '除了', '方好', '肠', '傍', '店', '令', '相逢', '叙', '足', '小女', '第二', '财', '查', '要', '提起', '住在', '转过', '暂', '转', '顾', '称', '二三', '坐在', '重重', '顿', '厚', '，', '随即', '见人', '休', '留下', '急急', '决', '理', '由', '三四', '角', '逢', '小小', '告', '楼下', '雪', '自有', '无人', '万一', '薄', '深深', '尽', '寻', '真', '古今', '若', '留', '难', '实', '亲生', '那时', '六', '走到', '路上', '后', '满', '含', '受', '贺喜', '旁人', '十分', '光', '这里', '硬', '遭', '特来', '绿', '为人', '置', '惊动', '轻', '象', '接', '何日', '而去']\n",
      "['今悄', '厄难', '三眼彪', '比月', '龙天看顾', '不肯抬', '今朝诉', '袁隐去', '抽攒', '心非安', '王府享', '车水', '兜定', '取参奉母', '儿来取', '只见徐', '唐敏道', '英立', '怕邬', '弟独', '浇蜡烛', '飞雁', '众奶母', '金螭虎纽', '谢家楼', '听凭窦', '眉笔', '瘫化', '一匹纱', '亦可惊', '舞凤', '外信', '不辞辛苦', '稹饳', '挥使', '司马臣', '人钦', '舋端', '胀了', '连春', '之癖', '兕觥济', '鞘来', '参汤', '奎璧舅', '青马', '衰母事', '步摇', '碧霄无路', '央金', '疾儿', '林之洋问', '欲备', '扯叶', '登北榜', '坛酒么', '欲见影难', '香芹', '声哩', '董青', '堵场', '杭州贩', '盟说誓', '嚼蜡矣', '助丧', '掷一掷', '梦梨道', '烫手', '卫人', '考定', '妈好', '眼越', '谦离', '驾鸾车', '普满大斋', '走笔', '仁智', '抓药', '玉案斜侵', '眼泡', '芳讳么', '画迹', '极易处', '曾降敕', '紧皱眉头', '侯府款', '洵未残', '个康', '番蕙姿', '惊来一壁趋', '皱起', '难忽', '师礼', '寅正便', '逛逛', '钦天监奏', '一盏', '著肉', '祸害人', '彩通']\n"
     ]
    }
   ],
   "source": [
    "df_sorted = sorted(df, key=df.get, reverse=True) \n",
    "print (df_sorted[0:300])\n",
    "print(df_sorted[-101:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file = open(\"df_sorted.txt\", \"w\", encoding=\"utf-8\")\n",
    "for c in df_sorted:\n",
    "    file.write(str(c) + \" \")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idf = {}\n",
    "\n",
    "for x in range(0, len(documents)):\n",
    "    for term in documents[x]:\n",
    "        idf[term] = math.log(len(documents)/df[term])\n",
    "        \n",
    "uniquewords = []\n",
    "for term in idf:\n",
    "    uniquewords.append(term)\n",
    "\n",
    "docvectors = []\n",
    "for x in range(0, len(documents)):\n",
    "    tf = {}\n",
    "    tfidf = {}\n",
    "    vector = []\n",
    "    for term in uniquewords:\n",
    "        tf[term] = documents[x].count(term)\n",
    "        tfidf[term] = tf[term]*idf[term]\n",
    "        vector.append(tfidf[term])\n",
    "    docvectors.append (vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosinesimilarity(vectorx, vectory):\n",
    "    total = 0\n",
    "    xsize = 0\n",
    "    ysize = 0\n",
    "    for i in range(0, len(vectorx)):\n",
    "        total = total + vectorx[i]*vectory[i]\n",
    "        xsize = xsize + vectorx[i]*vectorx[i]\n",
    "        ysize = ysize + vectory[i]*vectory[i]\n",
    "    xsize = math.sqrt(xsize)\n",
    "    ysize = math.sqrt(ysize)\n",
    "    return total/xsize/ysize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph {\n",
      " 再生缘tokenized.txt -- 娇红记tokenized.txt [weight=0.004433975096373243]\n",
      " 再生缘tokenized.txt -- 平山冷燕tokenized.txt [weight=0.023457827116073464]\n",
      " 再生缘tokenized.txt -- 玉娇梨tokenized.txt [weight=0.009629503786104072]\n",
      " 再生缘tokenized.txt -- 红楼梦tokenized.txt [weight=0.016704617470099544]\n",
      " 再生缘tokenized.txt -- 醒世姻缘传tokenized.txt [weight=0.01434650714390784]\n",
      " 再生缘tokenized.txt -- 金云翘传tokenized.txt [weight=0.012803953600457193]\n",
      " 再生缘tokenized.txt -- 金瓶梅tokenized.txt [weight=0.01426032771824912]\n",
      " 再生缘tokenized.txt -- 镜花缘tokenized.txt [weight=0.038151652083748676]\n",
      " 再生缘tokenized.txt -- 鼓掌绝尘tokenized.txt [weight=0.03843151203525029]\n",
      " 娇红记tokenized.txt -- 再生缘tokenized.txt [weight=0.004433975096373243]\n",
      " 娇红记tokenized.txt -- 平山冷燕tokenized.txt [weight=0.0008646995768443769]\n",
      " 娇红记tokenized.txt -- 玉娇梨tokenized.txt [weight=0.0003647177387729248]\n",
      " 娇红记tokenized.txt -- 红楼梦tokenized.txt [weight=0.0011699967461899366]\n",
      " 娇红记tokenized.txt -- 醒世姻缘传tokenized.txt [weight=0.0024057288838486984]\n",
      " 娇红记tokenized.txt -- 金云翘传tokenized.txt [weight=0.0008680102144328561]\n",
      " 娇红记tokenized.txt -- 金瓶梅tokenized.txt [weight=0.001987716164953931]\n",
      " 娇红记tokenized.txt -- 镜花缘tokenized.txt [weight=0.005615526183650598]\n",
      " 娇红记tokenized.txt -- 鼓掌绝尘tokenized.txt [weight=0.0017997697166707675]\n",
      " 平山冷燕tokenized.txt -- 再生缘tokenized.txt [weight=0.023457827116073467]\n",
      " 平山冷燕tokenized.txt -- 娇红记tokenized.txt [weight=0.0008646995768443769]\n",
      " 平山冷燕tokenized.txt -- 玉娇梨tokenized.txt [weight=0.00548020035191616]\n",
      " 平山冷燕tokenized.txt -- 红楼梦tokenized.txt [weight=0.002639232210582284]\n",
      " 平山冷燕tokenized.txt -- 醒世姻缘传tokenized.txt [weight=0.005983958796935928]\n",
      " 平山冷燕tokenized.txt -- 金云翘传tokenized.txt [weight=0.028658312735749435]\n",
      " 平山冷燕tokenized.txt -- 金瓶梅tokenized.txt [weight=0.003208449120055845]\n",
      " 平山冷燕tokenized.txt -- 镜花缘tokenized.txt [weight=0.009657350342625416]\n",
      " 平山冷燕tokenized.txt -- 鼓掌绝尘tokenized.txt [weight=0.006442237262834617]\n",
      " 玉娇梨tokenized.txt -- 再生缘tokenized.txt [weight=0.009629503786104074]\n",
      " 玉娇梨tokenized.txt -- 娇红记tokenized.txt [weight=0.0003647177387729248]\n",
      " 玉娇梨tokenized.txt -- 平山冷燕tokenized.txt [weight=0.00548020035191616]\n",
      " 玉娇梨tokenized.txt -- 红楼梦tokenized.txt [weight=0.002143095612321324]\n",
      " 玉娇梨tokenized.txt -- 醒世姻缘传tokenized.txt [weight=0.00496142314499284]\n",
      " 玉娇梨tokenized.txt -- 金云翘传tokenized.txt [weight=0.0020328657005175]\n",
      " 玉娇梨tokenized.txt -- 金瓶梅tokenized.txt [weight=0.003914451956839568]\n",
      " 玉娇梨tokenized.txt -- 镜花缘tokenized.txt [weight=0.00455743647619045]\n",
      " 玉娇梨tokenized.txt -- 鼓掌绝尘tokenized.txt [weight=0.007311160277229271]\n",
      " 红楼梦tokenized.txt -- 再生缘tokenized.txt [weight=0.016704617470099544]\n",
      " 红楼梦tokenized.txt -- 娇红记tokenized.txt [weight=0.0011699967461899366]\n",
      " 红楼梦tokenized.txt -- 平山冷燕tokenized.txt [weight=0.002639232210582284]\n",
      " 红楼梦tokenized.txt -- 玉娇梨tokenized.txt [weight=0.002143095612321324]\n",
      " 红楼梦tokenized.txt -- 醒世姻缘传tokenized.txt [weight=0.02055020423954272]\n",
      " 红楼梦tokenized.txt -- 金云翘传tokenized.txt [weight=0.010762672120163339]\n",
      " 红楼梦tokenized.txt -- 金瓶梅tokenized.txt [weight=0.012392768973470702]\n",
      " 红楼梦tokenized.txt -- 镜花缘tokenized.txt [weight=0.0137813399748743]\n",
      " 红楼梦tokenized.txt -- 鼓掌绝尘tokenized.txt [weight=0.009750801760519854]\n",
      " 醒世姻缘传tokenized.txt -- 再生缘tokenized.txt [weight=0.01434650714390784]\n",
      " 醒世姻缘传tokenized.txt -- 娇红记tokenized.txt [weight=0.0024057288838486984]\n",
      " 醒世姻缘传tokenized.txt -- 平山冷燕tokenized.txt [weight=0.005983958796935928]\n",
      " 醒世姻缘传tokenized.txt -- 玉娇梨tokenized.txt [weight=0.004961423144992841]\n",
      " 醒世姻缘传tokenized.txt -- 红楼梦tokenized.txt [weight=0.02055020423954272]\n",
      " 醒世姻缘传tokenized.txt -- 金云翘传tokenized.txt [weight=0.006618461037837992]\n",
      " 醒世姻缘传tokenized.txt -- 金瓶梅tokenized.txt [weight=0.029476617685056224]\n",
      " 醒世姻缘传tokenized.txt -- 镜花缘tokenized.txt [weight=0.021199176536475135]\n",
      " 醒世姻缘传tokenized.txt -- 鼓掌绝尘tokenized.txt [weight=0.016577735056634794]\n",
      " 金云翘传tokenized.txt -- 再生缘tokenized.txt [weight=0.01280395360045719]\n",
      " 金云翘传tokenized.txt -- 娇红记tokenized.txt [weight=0.0008680102144328562]\n",
      " 金云翘传tokenized.txt -- 平山冷燕tokenized.txt [weight=0.028658312735749432]\n",
      " 金云翘传tokenized.txt -- 玉娇梨tokenized.txt [weight=0.0020328657005175]\n",
      " 金云翘传tokenized.txt -- 红楼梦tokenized.txt [weight=0.010762672120163337]\n",
      " 金云翘传tokenized.txt -- 醒世姻缘传tokenized.txt [weight=0.006618461037837992]\n",
      " 金云翘传tokenized.txt -- 金瓶梅tokenized.txt [weight=0.0058602033676847165]\n",
      " 金云翘传tokenized.txt -- 镜花缘tokenized.txt [weight=0.00583534224414237]\n",
      " 金云翘传tokenized.txt -- 鼓掌绝尘tokenized.txt [weight=0.00678149939376639]\n",
      " 金瓶梅tokenized.txt -- 再生缘tokenized.txt [weight=0.01426032771824912]\n",
      " 金瓶梅tokenized.txt -- 娇红记tokenized.txt [weight=0.001987716164953931]\n",
      " 金瓶梅tokenized.txt -- 平山冷燕tokenized.txt [weight=0.0032084491200558445]\n",
      " 金瓶梅tokenized.txt -- 玉娇梨tokenized.txt [weight=0.003914451956839568]\n",
      " 金瓶梅tokenized.txt -- 红楼梦tokenized.txt [weight=0.012392768973470702]\n",
      " 金瓶梅tokenized.txt -- 醒世姻缘传tokenized.txt [weight=0.029476617685056224]\n",
      " 金瓶梅tokenized.txt -- 金云翘传tokenized.txt [weight=0.005860203367684717]\n",
      " 金瓶梅tokenized.txt -- 镜花缘tokenized.txt [weight=0.012869566378712713]\n",
      " 金瓶梅tokenized.txt -- 鼓掌绝尘tokenized.txt [weight=0.014131278734940706]\n",
      " 镜花缘tokenized.txt -- 再生缘tokenized.txt [weight=0.03815165208374867]\n",
      " 镜花缘tokenized.txt -- 娇红记tokenized.txt [weight=0.005615526183650599]\n",
      " 镜花缘tokenized.txt -- 平山冷燕tokenized.txt [weight=0.009657350342625416]\n",
      " 镜花缘tokenized.txt -- 玉娇梨tokenized.txt [weight=0.00455743647619045]\n",
      " 镜花缘tokenized.txt -- 红楼梦tokenized.txt [weight=0.013781339974874301]\n",
      " 镜花缘tokenized.txt -- 醒世姻缘传tokenized.txt [weight=0.021199176536475135]\n",
      " 镜花缘tokenized.txt -- 金云翘传tokenized.txt [weight=0.00583534224414237]\n",
      " 镜花缘tokenized.txt -- 金瓶梅tokenized.txt [weight=0.012869566378712713]\n",
      " 镜花缘tokenized.txt -- 鼓掌绝尘tokenized.txt [weight=0.013975369192363373]\n",
      " 鼓掌绝尘tokenized.txt -- 再生缘tokenized.txt [weight=0.0384315120352503]\n",
      " 鼓掌绝尘tokenized.txt -- 娇红记tokenized.txt [weight=0.0017997697166707675]\n",
      " 鼓掌绝尘tokenized.txt -- 平山冷燕tokenized.txt [weight=0.006442237262834617]\n",
      " 鼓掌绝尘tokenized.txt -- 玉娇梨tokenized.txt [weight=0.007311160277229271]\n",
      " 鼓掌绝尘tokenized.txt -- 红楼梦tokenized.txt [weight=0.009750801760519854]\n",
      " 鼓掌绝尘tokenized.txt -- 醒世姻缘传tokenized.txt [weight=0.016577735056634794]\n",
      " 鼓掌绝尘tokenized.txt -- 金云翘传tokenized.txt [weight=0.00678149939376639]\n",
      " 鼓掌绝尘tokenized.txt -- 金瓶梅tokenized.txt [weight=0.014131278734940706]\n",
      " 鼓掌绝尘tokenized.txt -- 镜花缘tokenized.txt [weight=0.013975369192363375]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(\"graph {\")  \n",
    "for x in range (0,len(docvectors)):\n",
    "    for y in range (0,len(docvectors)):\n",
    "        if x != y:\n",
    "            print (\" \" + str(documentnames[x]) + \" -- \" + str(documentnames[y]) + \" [weight=\" + str(cosinesimilarity(docvectors[x], docvectors[y])) + \"]\")       \n",
    "print(\"}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
